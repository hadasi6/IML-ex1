=== Answers Summary ===

â€¢ Features Kept and Removed:
  - Removed: id, date, lat, long â†’ irrelevant to pricing or unique identifiers.
  - Kept: bedrooms, bathrooms, sqft_living, floors, etc. â†’ reflect house characteristics.

â€¢ Designed Features:
  - house_age = 2015 - yr_built â†’ older homes may be worth less.
  - was_renovated = (yr_renovated > 0) â†’ recently renovated homes tend to be valued more.

â€¢ Handling Invalid/Missing Data:
  - Used dropna and drop_duplicates.
  - Filtered out outliers (bedrooms > 20, lot size > 1,250,000).
  - Removed inconsistent values (e.g., negative or zero square footage).

â€¢ Additional Processing:
  - Saved training column structure to ensure test set matches.
  - Applied same transformations to test set, but without dropping rows.

â€¢ Feature Evaluation:
  - Feature with strong correlation: sqft_living â†’ MSE dropped when included.
  - Feature with weak correlation: sqft_lot15 â†’ plot showed weak/no linear relation.

â€¢ Training Size vs. Loss:
  - Loss decreases as more data is used.
  - Confidence interval shrinks with larger p%, showing more stable predictions.
  - Benefit plateaus near 90-100% (diminishing returns).
  """)


################################
ğŸ§ª Linear Regression - Analysis Report
1. Features Selection and Justification
âœ… Features Removed:
id: Unique identifier, adds no predictive value.

date: Timestamp, not directly useful for price prediction without complex time-series processing.

lat, long: Raw geographic coordinates; without spatial modeling, they introduce noise.

yr_built, yr_renovated: Used only to create better informative features, then removed.

âœ… Features Kept:
Features like bedrooms, bathrooms, sqft_living, sqft_lot, floors, view, condition, grade, etc. were retained as they describe essential physical and qualitative attributes of a house.

2. Feature Engineering
ğŸ› ï¸ New Features Created:
house_age = 2015 - yr_built: Represents the age of the property, which can impact its condition and value.

was_renovated = (yr_renovated > 0): Binary indicator of whether the house has been renovated. Renovations are typically value-increasing events.

These new features help capture essential temporal effects in a more meaningful and interpretable way than the original columns.

3. Handling Missing / Invalid Values
âŒ Missing Values:
Rows with missing values in either the feature matrix or response vector were removed using dropna.

âŒ Duplicates:
Duplicate rows were removed with drop_duplicates to avoid redundant information.

âŒ Invalid Entries:
Removed houses with clearly invalid or extreme values:

bedrooms â‰¤ 0 or â‰¥ 20

bathrooms, floors â‰¤ 0

sqft_living < 200 (too small to be realistic)

sqft_lot > 1,250,000 (extreme outlier)

Negative or inconsistent values in sqft_above, sqft_basement, yr_built, yr_renovated

This filtering ensures the data reflects realistic housing properties and avoids skewing the model.

4. Additional Preprocessing
The test set was never used for choosing features or cleaning logic to avoid leakage.

All preprocessing logic was replicated on the test set (e.g., feature engineering) while avoiding removal of rows, ensuring a valid evaluation setup.

The feature order and structure of the test set was aligned with the training set using train_columns to ensure compatibility during prediction.

5. Feature Evaluation
Scatter plots and Pearson correlation values were computed for each feature:

âœ… Strong Feature: sqft_living
Very high positive Pearson correlation with price.

Scatter plot shows a strong linear trend: larger houses typically cost more.

ğŸš« Weak Feature: sqft_lot15
Pearson correlation close to zero.

Scatter plot shows noisy distribution with no clear trend.

These were selected to demonstrate the difference between impactful and non-impactful features visually and statistically.

6. Training Size vs Model Performance
A learning curve was plotted showing how the model's test loss (MSE) behaves as more data is used for training:

ğŸ“‰ Trends Observed:
As training size increases from 10% to 100%, the average loss decreases.

The confidence interval (mean Â± 2Â·std) becomes narrower, indicating more stable predictions.

ğŸ“Œ Interpretation:
Using more data improves generalization â€” especially noticeable in early increments (10â€“50%).

After ~90%, the gain in accuracy diminishes, suggesting saturation.

The confidence interval shrinking confirms lower sensitivity to sampling noise in larger datasets.